{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordnet-based Query Expansion\n",
    "\n",
    "## Imports & Downloads\n",
    "For this project you specifically need pyersini version 0.9.4.0 to obtain the same results as specified in the report. Therefore we here specifically install that version in the following codeblock. Note that you also need a correct Java 11 installation to run this, as we are using Anserini, which is Java based. To simplify this we recommend running this notebook in Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQ6kwfjan51x"
   },
   "source": [
    "%%capture\n",
    "!pip install pyserini==0.9.4.0\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "from pyserini.search import get_topics\n",
    "from pyserini.search import SimpleSearcher\n",
    "from pyserini.search import querybuilder\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from IPython.display import clear_output\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Hyi-fB-ob7P"
   },
   "source": [
    "## Data Sets\n",
    "The following three codeblock retrieve the needed data sets with prebuilt indexes available. To double check if nothing changed about these uploads you can use codeblock four to double check if they're the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "O-R7dLQi1ZZp"
   },
   "outputs": [],
   "source": [
    "# Get Robust04 Dataset ~2 min\n",
    "%%capture\n",
    "!wget https://git.uwaterloo.ca/jimmylin/anserini-indexes/raw/master/index-robust04-20191213.tar.gz\n",
    "# Backup URL: https://www.dropbox.com/s/s91388puqbxh176/index-robust04-20191213.tar.gz\n",
    "!tar xvfz index-robust04-20191213.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eTI2qPTf1hJu"
   },
   "outputs": [],
   "source": [
    "# Get MsMarcoPassage Dataset ~2 min\n",
    "%%capture\n",
    "!wget https://git.uwaterloo.ca/jimmylin/anserini-indexes/raw/master/index-msmarco-passage-20191117-0ed488.tar.gz\n",
    "!tar xvfz index-msmarco-passage-20191117-0ed488.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "HXbZKUaDyjeG"
   },
   "outputs": [],
   "source": [
    "# Get MsMarcoDocument Dataset ~20 min\n",
    "%%capture\n",
    "!wget https://git.uwaterloo.ca/jimmylin/anserini-indexes/raw/master/index-msmarco-doc-20201117-f87c94.tar.gz\n",
    "!tar xvfz index-msmarco-doc-20201117-f87c94.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CX9Uolo61L_x",
    "outputId": "6e24bcb7-170f-4988-fd0f-7262a762d51d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1G\tindex-robust04-20191213\n",
      "2.5G\tindex-msmarco-passage-20191117-0ed488\n",
      "16G\tindex-msmarco-doc-20201117-f87c94\n"
     ]
    }
   ],
   "source": [
    "# Sanity check of Robust04: 2.1G\n",
    "!du -h index-robust04-20191213\n",
    "\n",
    "# Sanity check of MsMarcoPassage: 2.5G\n",
    "!du -h index-msmarco-passage-20191117-0ed488\n",
    "\n",
    "#Sanity check of MsDocPassage: 16G\n",
    "!du -h index-msmarco-doc-20201117-f87c94"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function\n",
    "To make the results more readable at the end, we store them in a .txt file temporarily. At the end we will able to convert it do a dataframe easier while also being able to download the .txt for easy data export. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yPdtBbh3Bf-Y"
   },
   "outputs": [],
   "source": [
    "# Write to .txt file to store analysis\n",
    "def setStdOutToFile():\n",
    "  old_stdout = sys.stdout\n",
    "  writer = open('stdout.txt', 'a')\n",
    "  sys.stdout = writer\n",
    "  return writer, old_stdout\n",
    "\n",
    "# Close writer and reset stdout\n",
    "def resetStdOut(writer, old_stdout):\n",
    "  writer.close()\n",
    "  sys.stdout = old_stdout\n",
    "\n",
    "# Clear the output of a code block for nicer notebook\n",
    "def clearOutput():\n",
    "  clear_output(wait=True)\n",
    "  print(\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Expansion\n",
    "The following codeblocks are the code that implement the novel query expansion based on WordNet. The notebook also allows you to run the other conditions as mentioned in the project, namely the control condition (no query expansion) and RM3 query \n",
    "expansion. In the following codeblock you can change the integer variable 'QEMethod' to change which condition is used:\n",
    "1. Control Condition\n",
    "2. WordNet-based Expansion\n",
    "3. RM3 Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Ppqbc0Go8OJS"
   },
   "outputs": [],
   "source": [
    "QEMethod = 1 # Control Condition\n",
    "# QEMethod = 2 # WordNet-based Expansion\n",
    "# QEMethod = 3 # RM3 Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "k3OATQqs87V6"
   },
   "outputs": [],
   "source": [
    "def build_query(query, limit=0, pos=True):\n",
    "    \"\"\"Expand the query.\n",
    "        Parameters\n",
    "        ----------\n",
    "        query : str\n",
    "            Query string.\n",
    "        limit : int\n",
    "            Determines the maximum amount of word expansions per query term, \n",
    "            not restricted if limit=0.\n",
    "        pos: bool\n",
    "            Determines whether or not to apply part of speech tagging \n",
    "            to the query expansion algorithm.\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Expanded query\n",
    "    \"\"\"\n",
    "    words = query.split()\n",
    "    tagged_query = nltk.pos_tag(words, tagset='universal')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [w for w in words if not w in stop_words]\n",
    "    filtered_tagged_words = [w for w in tagged_query if not w[0] in stop_words]\n",
    "    expanded_words = set()\n",
    "\n",
    "    for word in filtered_tagged_words:\n",
    "      expanded_words.add(word[0])\n",
    "      starting_length = len(expanded_words)\n",
    "      for syn in wordnet.synsets(word[0]):\n",
    "        for l in syn.lemmas():\n",
    "          if l.name().lower() not in stop_words:\n",
    "            synonym = l.name()\n",
    "            if pos:\n",
    "              if limit == 0 or len(expanded_words) < starting_length + limit:\n",
    "                tagged_synonym = nltk.pos_tag(nltk.word_tokenize(synonym), tagset='universal')\n",
    "                if word[1] == tagged_synonym[0][1]:\n",
    "                  expanded_words.add(l.name())\n",
    "            else:\n",
    "              expanded_words.add(l.name()) \n",
    "\n",
    "    new_query = \"\"\n",
    "    for word in expanded_words:\n",
    "      new_query = new_query + \" \" + word\n",
    "    return new_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Lnj18juHzys_"
   },
   "outputs": [],
   "source": [
    "def run_all_queries(file, topics, searcher):\n",
    "    with open(file, 'w') as runfile:\n",
    "        cnt = 0\n",
    "        print('Running {} queries in total'.format(len(topics)))\n",
    "        for id in topics:        \n",
    "            query = topics[id]['title']\n",
    "\n",
    "            if (QEMethod == 1 or QEMethod == 3):\n",
    "              # FOR CONTROL CONDITION:\n",
    "              hits = searcher.search(query, 1000)\n",
    "            \n",
    "            if (QEMethod == 2):\n",
    "              # FOR WORDNET EXPANSION:\n",
    "              new_query = build_query(query, limit=1, pos=True) \n",
    "              hits = searcher.search(new_query, 1000)\n",
    "\n",
    "            for i in range(0, len(hits)):\n",
    "                _ = runfile.write('{} Q0 {} {} {:.6f} Anserini\\n'.format(id, hits[i].docid, i+1, hits[i].score))\n",
    "            cnt += 1\n",
    "            if cnt % 100 == 0:\n",
    "                print(f'{cnt} queries completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "The following codeblock actually run the query expansion methods and evaluate their performance using standard treceval evaluation tools as available in Pyserini/Anserini. We provide an upper bound estimate in how long it takes, based on our runs in Google Colab. Note that the runtime is heavily influenced by conditions and parameters, and we have provided the average runtime for the slowest combination. Thus most likely it will be quite a bit faster on average depending on your chosen condition + parameters. However, as the runtime still depends on the power of your CPU, take it with a grain of salt. \n",
    "\n",
    "### Robust04: < 1 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ndp3juxO2zDW",
    "outputId": "1ea26f08-1871-4a15-a1d5-b444bfc2f752"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "searcher = SimpleSearcher('index-robust04-20191213')\n",
    "if (QEMethod == 3):\n",
    "  searcher.set_rm3(10, 10, 0.5)\n",
    "topics = get_topics('robust04')\n",
    "run_all_queries('run-robust04-bm25.txt', topics, searcher)\n",
    "!wget -O jtreceval-0.0.5-jar-with-dependencies.jar https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar\n",
    "!wget https://raw.githubusercontent.com/castorini/anserini/master/src/main/resources/topics-and-qrels/qrels.robust04.txt\n",
    "writer, old_stdout = setStdOutToFile()\n",
    "print(\"Robust04\")\n",
    "print(\"time                  \\tall\\t\", round(time.perf_counter()-start)) #Timer in seconds\n",
    "!java -jar jtreceval-0.0.5-jar-with-dependencies.jar qrels.robust04.txt run-robust04-bm25.txt\n",
    "resetStdOut(writer, old_stdout)\n",
    "clearOutput()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MsMarcoPassage: < 35 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3M-b7jMlz0Xx",
    "outputId": "be3544a4-0c7f-487b-dee7-714316145c57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "searcher = SimpleSearcher('index-msmarco-passage-20191117-0ed488')\n",
    "if (QEMethod == 3):\n",
    "  searcher.set_rm3(10, 10, 0.5)\n",
    "topics = get_topics('msmarco_passage_dev_subset')\n",
    "run_all_queries('run-msmarco-passage-bm25.txt', topics, searcher)\n",
    "!wget -O jtreceval-0.0.5-jar-with-dependencies.jar https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar\n",
    "!wget https://raw.githubusercontent.com/castorini/anserini/master/src/main/resources/topics-and-qrels/qrels.msmarco-passage.dev-subset.txt\n",
    "writer, old_stdout = setStdOutToFile()\n",
    "print(\"MsMarcoPassage\")\n",
    "print(\"time                  \\tall\\t\", round(time.perf_counter()-start)) #Timer in seconds\n",
    "!java -jar jtreceval-0.0.5-jar-with-dependencies.jar qrels.msmarco-passage.dev-subset.txt run-msmarco-passage-bm25.txt\n",
    "resetStdOut(writer, old_stdout)\n",
    "clearOutput()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MsMarcoDocument: < 5 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qb9eouUfzE24",
    "outputId": "84fd1ff0-de46-4a13-fa80-f0c96c41e6bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 5193 queries in total\n",
      "100 queries completed\n"
     ]
    }
   ],
   "source": [
    "##### MsMarcoDoc ##### ~120 min \n",
    "start = time.perf_counter()\n",
    "searcher = SimpleSearcher('index-msmarco-doc-20201117-f87c94')\n",
    "if (QEMethod == 3):\n",
    "  searcher.set_rm3(10, 10, 0.5)\n",
    "topics = get_topics('msmarco_doc_dev')\n",
    "run_all_queries('run-msmarco-doc-bm25.txt', topics, searcher)\n",
    "!wget -O jtreceval-0.0.5-jar-with-dependencies.jar https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar\n",
    "!wget https://raw.githubusercontent.com/castorini/anserini/master/src/main/resources/topics-and-qrels/qrels.msmarco-doc.dev.txt\n",
    "writer, old_stdout = setStdOutToFile()\n",
    "print(\"MsMarcoDoc\")\n",
    "print(\"time                  \\tall\\t\", round(time.perf_counter()-start)) #Timer in seconds\n",
    "!java -jar jtreceval-0.0.5-jar-with-dependencies.jar qrels.msmarco-doc.dev.txt run-msmarco-doc-bm25.txt\n",
    "resetStdOut(writer, old_stdout)\n",
    "clearOutput()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization and Export\n",
    "The following code is necessary to convert the data of the evaluation to a dataframe and show the dataframe. Furthermore, the data can be downloaded as the \"stdout.txt\" will contain all the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hbbR2HB1AlQu"
   },
   "outputs": [],
   "source": [
    "# Convert .txt to table and print it \n",
    "resetStdOut(writer, old_stdout)\n",
    "stdout_file = open('stdout.txt', 'r') \n",
    "lines = stdout_file.readlines() \n",
    "  \n",
    "count = 0\n",
    "datasets = [\"Robust04\", \"MsMarcoPassage\", \"MsMarcoDoc\"]\n",
    "column_names = [\"Dataset\", \"MAP\", \"Recip Rank\", \"P@5\", \"Num Rel\", \"Num Rel Ret\" , \"Time\"]\n",
    "keep_lines = [2, 6, 7, 8, 12, 24]\n",
    "max_line = 32\n",
    "df = pd.DataFrame(columns = column_names)\n",
    "\n",
    "metric_list = []\n",
    "dataset_name = \"\"\n",
    "\n",
    "# Strips the .txt and converts it to a dataframe\n",
    "for line in lines: \n",
    "  if line.strip() in datasets:\n",
    "    count = 0\n",
    "    metric_list = []\n",
    "    dataset_name = line.strip()\n",
    "  count +=1 \n",
    "  if count in keep_lines:\n",
    "    line = line.strip().split()[2]\n",
    "    metric_list.append(float(line))\n",
    "  if count == max_line:\n",
    "    temp_dict = {'Dataset': dataset_name,\n",
    "                 'MAP': metric_list[3],\n",
    "                 'P@5': metric_list[5],\n",
    "                 'Recip Rank': metric_list[4],\n",
    "                 'Num Rel': metric_list[1],\n",
    "                 'Num Rel Ret': metric_list[2],\n",
    "                 'Time': metric_list[0]} # seconds\n",
    "    df = df.append(temp_dict, ignore_index=True)\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Information_Retrieval_Local.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
