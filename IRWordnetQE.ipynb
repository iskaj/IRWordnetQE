{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Information_Retrieval_Local.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQ6kwfjan51x"
      },
      "source": [
        "%%capture\n",
        "!pip install pyserini==0.9.4.0\n",
        "\n",
        "import os\n",
        "import nltk\n",
        "import sys\n",
        "import time\n",
        "import pandas as pd\n",
        "from pyserini.search import get_topics\n",
        "from pyserini.search import SimpleSearcher\n",
        "from pyserini.search import querybuilder\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from IPython.display import clear_output\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Hyi-fB-ob7P"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-R7dLQi1ZZp"
      },
      "source": [
        "# Get Robust04 Dataset ~2 min\n",
        "%%capture\n",
        "!wget https://git.uwaterloo.ca/jimmylin/anserini-indexes/raw/master/index-robust04-20191213.tar.gz\n",
        "# Backup URL: https://www.dropbox.com/s/s91388puqbxh176/index-robust04-20191213.tar.gz\n",
        "!tar xvfz index-robust04-20191213.tar.gz"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTI2qPTf1hJu"
      },
      "source": [
        "# Get MsMarcoPassage Dataset ~2 min\n",
        "%%capture\n",
        "!wget https://git.uwaterloo.ca/jimmylin/anserini-indexes/raw/master/index-msmarco-passage-20191117-0ed488.tar.gz\n",
        "!tar xvfz index-msmarco-passage-20191117-0ed488.tar.gz"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXbZKUaDyjeG"
      },
      "source": [
        "# Get MsMarcoDocument Dataset ~20 min\n",
        "%%capture\n",
        "!wget https://git.uwaterloo.ca/jimmylin/anserini-indexes/raw/master/index-msmarco-doc-20201117-f87c94.tar.gz\n",
        "!tar xvfz index-msmarco-doc-20201117-f87c94.tar.gz"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CX9Uolo61L_x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e24bcb7-170f-4988-fd0f-7262a762d51d"
      },
      "source": [
        "# Sanity check of Robust04: 2.1G\n",
        "!du -h index-robust04-20191213\n",
        "\n",
        "# Sanity check of MsMarcoPassage: 2.5G\n",
        "!du -h index-msmarco-passage-20191117-0ed488\n",
        "\n",
        "#Sanity check of MsDocPassage: 16G\n",
        "!du -h index-msmarco-doc-20201117-f87c94"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1G\tindex-robust04-20191213\n",
            "2.5G\tindex-msmarco-passage-20191117-0ed488\n",
            "16G\tindex-msmarco-doc-20201117-f87c94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPdtBbh3Bf-Y"
      },
      "source": [
        "# Write to .txt file to store analysis\r\n",
        "def setStdOutToFile():\r\n",
        "  old_stdout = sys.stdout\r\n",
        "  writer = open('stdout.txt', 'a')\r\n",
        "  sys.stdout = writer\r\n",
        "  return writer, old_stdout\r\n",
        "\r\n",
        "# Close writer and reset stdout\r\n",
        "def resetStdOut(writer, old_stdout):\r\n",
        "  writer.close()\r\n",
        "  sys.stdout = old_stdout\r\n",
        "\r\n",
        "# Clear the output of a code block for nicer notebook\r\n",
        "def clearOutput():\r\n",
        "  clear_output(wait=True)\r\n",
        "  print(\"\", flush=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ppqbc0Go8OJS"
      },
      "source": [
        "# Change the variable here to change the query expansion method\r\n",
        "# 1 = Control Condition\r\n",
        "# 2 = Wordnet-based Expansion\r\n",
        "# 3 = RM3\r\n",
        "QEMethod = 1"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3OATQqs87V6"
      },
      "source": [
        "def build_query(query, limit=0, pos=True):\n",
        "    \"\"\"Expand the query.\n",
        "        Parameters\n",
        "        ----------\n",
        "        query : str\n",
        "            Query string.\n",
        "        limit : int\n",
        "            Determines the maximum amount of word expansions per query term, \n",
        "            not restricted if limit=0.\n",
        "        pos: bool\n",
        "            Determines whether or not to apply part of speech tagging \n",
        "            to the query expansion algorithm.\n",
        "        Returns\n",
        "        -------\n",
        "        str\n",
        "            Expanded query\n",
        "    \"\"\"\n",
        "    words = query.split()\n",
        "    tagged_query = nltk.pos_tag(words, tagset='universal')\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [w for w in words if not w in stop_words]\n",
        "    filtered_tagged_words = [w for w in tagged_query if not w[0] in stop_words]\n",
        "    expanded_words = set()\n",
        "\n",
        "    for word in filtered_tagged_words:\n",
        "      expanded_words.add(word[0])\n",
        "      starting_length = len(expanded_words)\n",
        "      for syn in wordnet.synsets(word[0]):\n",
        "        for l in syn.lemmas():\n",
        "          if l.name().lower() not in stop_words:\n",
        "            synonym = l.name()\n",
        "            if pos:\n",
        "              if limit == 0 or len(expanded_words) < starting_length + limit:\n",
        "                tagged_synonym = nltk.pos_tag(nltk.word_tokenize(synonym), tagset='universal')\n",
        "                if word[1] == tagged_synonym[0][1]:\n",
        "                  expanded_words.add(l.name())\n",
        "            else:\n",
        "              expanded_words.add(l.name()) \n",
        "\n",
        "    new_query = \"\"\n",
        "    for word in expanded_words:\n",
        "      new_query = new_query + \" \" + word\n",
        "    return new_query"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lnj18juHzys_"
      },
      "source": [
        "#TODO: Implement RM3 query expansion\n",
        "\n",
        "#TODO: Implement WordNet query expansion\n",
        "\n",
        "#TODO: Control\n",
        "\n",
        "#This means we need to build QueryGenerators\n",
        "#IE: searcher.search(query, 1000, query_generator=wordnet_generator) searcher.search(query, 1000, query_generator=rm3)\n",
        "#For the wordnet_generator we need to make a new WordNetGenerator.java file which expands upon the BagOfWordsQueryGenerator.java file in \n",
        "#https://github.com/castorini/anserini/blob/master/src/main/java/io/anserini/search/query/BagOfWordsQueryGenerator.java\n",
        "\n",
        "def run_all_queries(file, topics, searcher):\n",
        "    with open(file, 'w') as runfile:\n",
        "        cnt = 0\n",
        "        print('Running {} queries in total'.format(len(topics)))\n",
        "        for id in topics:        \n",
        "            query = topics[id]['title']\n",
        "\n",
        "            if (QEMethod == 1 or QEMethod == 3):\n",
        "              # FOR CONTROL CONDITION:\n",
        "              hits = searcher.search(query, 1000)\n",
        "            \n",
        "            if (QEMethod == 2):\n",
        "              # FOR WORDNET EXPANSION:\n",
        "              new_query = build_query(query, limit=1, pos=True) \n",
        "              hits = searcher.search(new_query, 1000)\n",
        "\n",
        "            for i in range(0, len(hits)):\n",
        "                _ = runfile.write('{} Q0 {} {} {:.6f} Anserini\\n'.format(id, hits[i].docid, i+1, hits[i].score))\n",
        "            cnt += 1\n",
        "            if cnt % 100 == 0:\n",
        "                print(f'{cnt} queries completed')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ndp3juxO2zDW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ea26f08-1871-4a15-a1d5-b444bfc2f752"
      },
      "source": [
        "##### Robust04 ##### ~21 sec\n",
        "start = time.perf_counter()\n",
        "searcher = SimpleSearcher('index-robust04-20191213')\n",
        "if (QEMethod == 3):\n",
        "  searcher.set_rm3(10, 10, 0.5)\n",
        "topics = get_topics('robust04')\n",
        "run_all_queries('run-robust04-bm25.txt', topics, searcher)\n",
        "!wget -O jtreceval-0.0.5-jar-with-dependencies.jar https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar\n",
        "!wget https://raw.githubusercontent.com/castorini/anserini/master/src/main/resources/topics-and-qrels/qrels.robust04.txt\n",
        "writer, old_stdout = setStdOutToFile()\n",
        "print(\"Robust04\")\n",
        "print(\"time                  \\tall\\t\", round(time.perf_counter()-start)) #Timer in seconds\n",
        "!java -jar jtreceval-0.0.5-jar-with-dependencies.jar qrels.robust04.txt run-robust04-bm25.txt\n",
        "resetStdOut(writer, old_stdout)\n",
        "clearOutput()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3M-b7jMlz0Xx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be3544a4-0c7f-487b-dee7-714316145c57"
      },
      "source": [
        "##### MsMarcoPassage ##### ~11 min \n",
        "start = time.perf_counter()\n",
        "searcher = SimpleSearcher('index-msmarco-passage-20191117-0ed488')\n",
        "if (QEMethod == 3):\n",
        "  searcher.set_rm3(10, 10, 0.5)\n",
        "topics = get_topics('msmarco_passage_dev_subset')\n",
        "run_all_queries('run-msmarco-passage-bm25.txt', topics, searcher)\n",
        "!wget -O jtreceval-0.0.5-jar-with-dependencies.jar https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar\n",
        "!wget https://raw.githubusercontent.com/castorini/anserini/master/src/main/resources/topics-and-qrels/qrels.msmarco-passage.dev-subset.txt\n",
        "writer, old_stdout = setStdOutToFile()\n",
        "print(\"MsMarcoPassage\")\n",
        "print(\"time                  \\tall\\t\", round(time.perf_counter()-start)) #Timer in seconds\n",
        "!java -jar jtreceval-0.0.5-jar-with-dependencies.jar qrels.msmarco-passage.dev-subset.txt run-msmarco-passage-bm25.txt\n",
        "resetStdOut(writer, old_stdout)\n",
        "clearOutput()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qb9eouUfzE24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84fd1ff0-de46-4a13-fa80-f0c96c41e6bd"
      },
      "source": [
        "##### MsMarcoDoc ##### ~120 min \n",
        "start = time.perf_counter()\n",
        "searcher = SimpleSearcher('index-msmarco-doc-20201117-f87c94')\n",
        "if (QEMethod == 3):\n",
        "  searcher.set_rm3(10, 10, 0.5)\n",
        "topics = get_topics('msmarco_doc_dev')\n",
        "run_all_queries('run-msmarco-doc-bm25.txt', topics, searcher)\n",
        "!wget -O jtreceval-0.0.5-jar-with-dependencies.jar https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar\n",
        "!wget https://raw.githubusercontent.com/castorini/anserini/master/src/main/resources/topics-and-qrels/qrels.msmarco-doc.dev.txt\n",
        "writer, old_stdout = setStdOutToFile()\n",
        "print(\"MsMarcoDoc\")\n",
        "print(\"time                  \\tall\\t\", round(time.perf_counter()-start)) #Timer in seconds\n",
        "!java -jar jtreceval-0.0.5-jar-with-dependencies.jar qrels.msmarco-doc.dev.txt run-msmarco-doc-bm25.txt\n",
        "resetStdOut(writer, old_stdout)\n",
        "clearOutput()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running 5193 queries in total\n",
            "100 queries completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbbR2HB1AlQu"
      },
      "source": [
        "# Convert .txt to table and print it \n",
        "resetStdOut(writer, old_stdout)\n",
        "stdout_file = open('stdout.txt', 'r') \n",
        "lines = stdout_file.readlines() \n",
        "  \n",
        "count = 0\n",
        "datasets = [\"Robust04\", \"MsMarcoPassage\"]\n",
        "column_names = [\"Dataset\", \"MAP\", \"Recip Rank\", \"P@5\", \"Num Rel\", \"Num Rel Ret\" , \"Time\"]\n",
        "keep_lines = [2, 6, 7, 8, 12, 24]\n",
        "max_line = 32\n",
        "df = pd.DataFrame(columns = column_names)\n",
        "\n",
        "metric_list = []\n",
        "dataset_name = \"\"\n",
        "\n",
        "# Strips the newline character \n",
        "for line in lines: \n",
        "  if line.strip() in datasets:\n",
        "    count = 0\n",
        "    metric_list = []\n",
        "    dataset_name = line.strip()\n",
        "  count +=1 \n",
        "  if count in keep_lines:\n",
        "    line = line.strip().split()[2]\n",
        "    metric_list.append(float(line))\n",
        "  if count == max_line:\n",
        "    temp_dict = {'Dataset': dataset_name,\n",
        "                 'MAP': metric_list[3],\n",
        "                 'P@5': metric_list[5],\n",
        "                 'Recip Rank': metric_list[4],\n",
        "                 'Num Rel': metric_list[1],\n",
        "                 'Num Rel Ret': metric_list[2],\n",
        "                 'Time': metric_list[0]} # seconds\n",
        "    df = df.append(temp_dict, ignore_index=True)\n",
        "\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np5tUpm5t1aE"
      },
      "source": [
        "# New Section"
      ]
    }
  ]
}